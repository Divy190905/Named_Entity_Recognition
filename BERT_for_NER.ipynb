{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A5fR1DFZvO1-"
      },
      "outputs": [],
      "source": [
        "!pip install transformers datasets tokenizers seqeval -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install evaluate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5LihKSXKwqjF",
        "outputId": "4c7c9797-ff67-49a7-d438-592efdfbc4ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.11/dist-packages (0.4.3)\n",
            "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.2.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from evaluate) (1.26.4)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.70.16)\n",
            "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.9.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.27.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from evaluate) (24.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.16.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (17.0.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.11.11)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2024.12.14)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2024.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.18.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import datasets\n",
        "import numpy as np\n",
        "from transformers import BertTokenizerFast\n",
        "from transformers import DataCollatorForTokenClassification\n",
        "from transformers import AutoModelForTokenClassification\n",
        "import evaluate\n",
        "\n",
        "conll2003 = datasets.load_dataset(\"conll2003\")"
      ],
      "metadata": {
        "id": "JT5PsNAMvuOE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conll2003"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e_n7OFwEvzN0",
        "outputId": "20320f26-a75e-446f-f4bf-026b9485bcde"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
              "        num_rows: 14041\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
              "        num_rows: 3250\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
              "        num_rows: 3453\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conll2003.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N5hnlvfSv2D9",
        "outputId": "0c9c9252-3493-42a4-a1e0-801137834bda"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'train': (14041, 5), 'validation': (3250, 5), 'test': (3453, 5)}"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conll2003[\"train\"][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4HYj-iXZv33s",
        "outputId": "b219c3fe-ab74-443e-cb08-3fd6a3ece699"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id': '0',\n",
              " 'tokens': ['EU',\n",
              "  'rejects',\n",
              "  'German',\n",
              "  'call',\n",
              "  'to',\n",
              "  'boycott',\n",
              "  'British',\n",
              "  'lamb',\n",
              "  '.'],\n",
              " 'pos_tags': [22, 42, 16, 21, 35, 37, 16, 21, 7],\n",
              " 'chunk_tags': [11, 21, 11, 12, 21, 22, 11, 12, 0],\n",
              " 'ner_tags': [3, 0, 7, 0, 0, 0, 7, 0, 0]}"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")"
      ],
      "metadata": {
        "id": "sZr0Nvmhv8Ww"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conll2003['train'][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kbra3T-nv_I6",
        "outputId": "2f49a0c7-d0a9-4a57-ff70-e53ce9beccab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id': '0',\n",
              " 'tokens': ['EU',\n",
              "  'rejects',\n",
              "  'German',\n",
              "  'call',\n",
              "  'to',\n",
              "  'boycott',\n",
              "  'British',\n",
              "  'lamb',\n",
              "  '.'],\n",
              " 'pos_tags': [22, 42, 16, 21, 35, 37, 16, 21, 7],\n",
              " 'chunk_tags': [11, 21, 11, 12, 21, 22, 11, 12, 0],\n",
              " 'ner_tags': [3, 0, 7, 0, 0, 0, 7, 0, 0]}"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "example_text = conll2003['train'][0]\n",
        "\n",
        "tokenized_input = tokenizer(example_text[\"tokens\"], is_split_into_words=True)\n",
        "\n",
        "tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n",
        "\n",
        "word_ids = tokenized_input.word_ids()\n",
        "\n",
        "print(word_ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GQ2gI5ZhwBRx",
        "outputId": "ba391cd0-163d-45f3-baaa-c11ff46a06af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[None, 0, 1, 2, 3, 4, 5, 6, 7, 8, None]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n",
        "tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jikOAHbPwEMy",
        "outputId": "bd4b0fb3-00c2-42a9-88b2-6064667ab9fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['[CLS]',\n",
              " 'eu',\n",
              " 'rejects',\n",
              " 'german',\n",
              " 'call',\n",
              " 'to',\n",
              " 'boycott',\n",
              " 'british',\n",
              " 'lamb',\n",
              " '.',\n",
              " '[SEP]']"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(example_text['ner_tags']), len(tokenized_input[\"input_ids\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KkgM0cP-wGIe",
        "outputId": "ee511774-768f-4957-ae75-ce95e810245c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(9, 11)"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def tokenize_and_align_labels(examples, label_all_tokens=True):\n",
        "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
        "    labels = []\n",
        "    for i, label in enumerate(examples[\"ner_tags\"]):\n",
        "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
        "        # word_ids() => Return a list mapping the tokens\n",
        "        # to their actual word in the initial sentence.\n",
        "        # It Returns a list indicating the word corresponding to each token.\n",
        "        previous_word_idx = None\n",
        "        label_ids = []\n",
        "        # Special tokens like `` and `<\\s>` are originally mapped to None\n",
        "        # We need to set the label to -100 so they are automatically ignored in the loss function.\n",
        "        for word_idx in word_ids:\n",
        "            if word_idx is None:\n",
        "                # set –100 as the label for these special tokens\n",
        "                label_ids.append(-100)\n",
        "            # For the other tokens in a word, we set the label to either the current label or -100, depending on\n",
        "            # the label_all_tokens flag.\n",
        "            elif word_idx != previous_word_idx:\n",
        "                # if current word_idx is != prev then its the most regular case\n",
        "                # and add the corresponding token\n",
        "                label_ids.append(label[word_idx])\n",
        "            else:\n",
        "                # to take care of sub-words which have the same word_idx\n",
        "                # set -100 as well for them, but only if label_all_tokens == False\n",
        "                label_ids.append(label[word_idx] if label_all_tokens else -100)\n",
        "                # mask the subword representations after the first subword\n",
        "\n",
        "            previous_word_idx = word_idx\n",
        "        labels.append(label_ids)\n",
        "    tokenized_inputs[\"labels\"] = labels\n",
        "    return tokenized_inputs"
      ],
      "metadata": {
        "id": "tM6cMuQWwIYy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conll2003['train'][4:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eMkbIxZRwK2-",
        "outputId": "ab9669fe-47b6-4976-914c-47e165235bd2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id': ['4'],\n",
              " 'tokens': [['Germany',\n",
              "   \"'s\",\n",
              "   'representative',\n",
              "   'to',\n",
              "   'the',\n",
              "   'European',\n",
              "   'Union',\n",
              "   \"'s\",\n",
              "   'veterinary',\n",
              "   'committee',\n",
              "   'Werner',\n",
              "   'Zwingmann',\n",
              "   'said',\n",
              "   'on',\n",
              "   'Wednesday',\n",
              "   'consumers',\n",
              "   'should',\n",
              "   'buy',\n",
              "   'sheepmeat',\n",
              "   'from',\n",
              "   'countries',\n",
              "   'other',\n",
              "   'than',\n",
              "   'Britain',\n",
              "   'until',\n",
              "   'the',\n",
              "   'scientific',\n",
              "   'advice',\n",
              "   'was',\n",
              "   'clearer',\n",
              "   '.']],\n",
              " 'pos_tags': [[22,\n",
              "   27,\n",
              "   21,\n",
              "   35,\n",
              "   12,\n",
              "   22,\n",
              "   22,\n",
              "   27,\n",
              "   16,\n",
              "   21,\n",
              "   22,\n",
              "   22,\n",
              "   38,\n",
              "   15,\n",
              "   22,\n",
              "   24,\n",
              "   20,\n",
              "   37,\n",
              "   21,\n",
              "   15,\n",
              "   24,\n",
              "   16,\n",
              "   15,\n",
              "   22,\n",
              "   15,\n",
              "   12,\n",
              "   16,\n",
              "   21,\n",
              "   38,\n",
              "   17,\n",
              "   7]],\n",
              " 'chunk_tags': [[11,\n",
              "   11,\n",
              "   12,\n",
              "   13,\n",
              "   11,\n",
              "   12,\n",
              "   12,\n",
              "   11,\n",
              "   12,\n",
              "   12,\n",
              "   12,\n",
              "   12,\n",
              "   21,\n",
              "   13,\n",
              "   11,\n",
              "   12,\n",
              "   21,\n",
              "   22,\n",
              "   11,\n",
              "   13,\n",
              "   11,\n",
              "   1,\n",
              "   13,\n",
              "   11,\n",
              "   17,\n",
              "   11,\n",
              "   12,\n",
              "   12,\n",
              "   21,\n",
              "   1,\n",
              "   0]],\n",
              " 'ner_tags': [[5,\n",
              "   0,\n",
              "   0,\n",
              "   0,\n",
              "   0,\n",
              "   3,\n",
              "   4,\n",
              "   0,\n",
              "   0,\n",
              "   0,\n",
              "   1,\n",
              "   2,\n",
              "   0,\n",
              "   0,\n",
              "   0,\n",
              "   0,\n",
              "   0,\n",
              "   0,\n",
              "   0,\n",
              "   0,\n",
              "   0,\n",
              "   0,\n",
              "   0,\n",
              "   5,\n",
              "   0,\n",
              "   0,\n",
              "   0,\n",
              "   0,\n",
              "   0,\n",
              "   0,\n",
              "   0]]}"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "q = tokenize_and_align_labels(conll2003['train'][4:5])\n",
        "print(q)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kj7uXluBwNIR",
        "outputId": "be1ec8e5-4fe6-4f12-dc67-ea2248359b91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': [[101, 2762, 1005, 1055, 4387, 2000, 1996, 2647, 2586, 1005, 1055, 15651, 2837, 14121, 1062, 9328, 5804, 2056, 2006, 9317, 10390, 2323, 4965, 8351, 4168, 4017, 2013, 3032, 2060, 2084, 3725, 2127, 1996, 4045, 6040, 2001, 24509, 1012, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'labels': [[-100, 5, 0, 0, 0, 0, 0, 3, 4, 0, 0, 0, 0, 1, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, -100]]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for token, label in zip(tokenizer.convert_ids_to_tokens(q[\"input_ids\"][0]),q[\"labels\"][0]):\n",
        "    print(f\"{token:_<40} {label}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J7GlutszwP_u",
        "outputId": "368e9220-1b9b-46e7-aaa8-33587ca7c553"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CLS]___________________________________ -100\n",
            "germany_________________________________ 5\n",
            "'_______________________________________ 0\n",
            "s_______________________________________ 0\n",
            "representative__________________________ 0\n",
            "to______________________________________ 0\n",
            "the_____________________________________ 0\n",
            "european________________________________ 3\n",
            "union___________________________________ 4\n",
            "'_______________________________________ 0\n",
            "s_______________________________________ 0\n",
            "veterinary______________________________ 0\n",
            "committee_______________________________ 0\n",
            "werner__________________________________ 1\n",
            "z_______________________________________ 2\n",
            "##wing__________________________________ 2\n",
            "##mann__________________________________ 2\n",
            "said____________________________________ 0\n",
            "on______________________________________ 0\n",
            "wednesday_______________________________ 0\n",
            "consumers_______________________________ 0\n",
            "should__________________________________ 0\n",
            "buy_____________________________________ 0\n",
            "sheep___________________________________ 0\n",
            "##me____________________________________ 0\n",
            "##at____________________________________ 0\n",
            "from____________________________________ 0\n",
            "countries_______________________________ 0\n",
            "other___________________________________ 0\n",
            "than____________________________________ 0\n",
            "britain_________________________________ 5\n",
            "until___________________________________ 0\n",
            "the_____________________________________ 0\n",
            "scientific______________________________ 0\n",
            "advice__________________________________ 0\n",
            "was_____________________________________ 0\n",
            "clearer_________________________________ 0\n",
            "._______________________________________ 0\n",
            "[SEP]___________________________________ -100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_datasets = conll2003.map(tokenize_and_align_labels, batched=True)"
      ],
      "metadata": {
        "id": "fblJNV0bwSKS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_datasets['train'][0]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qbGcyRXUwUWZ",
        "outputId": "8bf49125-73ff-4b3c-8ef7-d3d58825853e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id': '0',\n",
              " 'tokens': ['EU',\n",
              "  'rejects',\n",
              "  'German',\n",
              "  'call',\n",
              "  'to',\n",
              "  'boycott',\n",
              "  'British',\n",
              "  'lamb',\n",
              "  '.'],\n",
              " 'pos_tags': [22, 42, 16, 21, 35, 37, 16, 21, 7],\n",
              " 'chunk_tags': [11, 21, 11, 12, 21, 22, 11, 12, 0],\n",
              " 'ner_tags': [3, 0, 7, 0, 0, 0, 7, 0, 0],\n",
              " 'input_ids': [101,\n",
              "  7327,\n",
              "  19164,\n",
              "  2446,\n",
              "  2655,\n",
              "  2000,\n",
              "  17757,\n",
              "  2329,\n",
              "  12559,\n",
              "  1012,\n",
              "  102],\n",
              " 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              " 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
              " 'labels': [-100, 3, 0, 7, 0, 0, 0, 7, 0, 0, -100]}"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForTokenClassification.from_pretrained(\"bert-base-uncased\", num_labels=9)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3S7lxEdbwW-T",
        "outputId": "90785fdb-9ef9-4e70-a200-9b5de83e1d2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "\n",
        "args = TrainingArguments(\n",
        "\"test-ner\",\n",
        "evaluation_strategy = \"epoch\",\n",
        "learning_rate=2e-5,\n",
        "per_device_train_batch_size=16,\n",
        "per_device_eval_batch_size=16,\n",
        "num_train_epochs=4,\n",
        "weight_decay=0.01,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ak7Li4dBwZhH",
        "outputId": "d904f9e1-47c4-47fe-a1d9-b7ffaad3e74b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_collator = DataCollatorForTokenClassification(tokenizer)"
      ],
      "metadata": {
        "id": "fNYXWrd3wcbp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metric = evaluate.load(\"seqeval\")"
      ],
      "metadata": {
        "id": "DOUD27F7we8U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example = conll2003['train'][0]"
      ],
      "metadata": {
        "id": "u1jFNqYMwgjR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_list = conll2003[\"train\"].features[\"ner_tags\"].feature.names\n",
        "\n",
        "label_list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E1A7H8nbxDOf",
        "outputId": "477d7677-6376-472a-9156-fff3bd767a90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for i in example[\"ner_tags\"]:\n",
        "  print(i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DMRvNumsxFd6",
        "outputId": "da010322-d17b-4d44-bf0b-406852d6df1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3\n",
            "0\n",
            "7\n",
            "0\n",
            "0\n",
            "0\n",
            "7\n",
            "0\n",
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "labels = [label_list[i] for i in example[\"ner_tags\"]]\n",
        "labels\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ogli_PZZxG71",
        "outputId": "5d8e363e-7e4b-4325-f53c-1db6da831e73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['B-ORG', 'O', 'B-MISC', 'O', 'O', 'O', 'B-MISC', 'O', 'O']"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def compute_metrics(eval_preds):\n",
        "    pred_logits, labels = eval_preds\n",
        "\n",
        "    pred_logits = np.argmax(pred_logits, axis=2)\n",
        "    # the logits and the probabilities are in the same order,\n",
        "    # so we don’t need to apply the softmax\n",
        "\n",
        "    # We remove all the values where the label is -100\n",
        "    predictions = [\n",
        "        [label_list[eval_preds] for (eval_preds, l) in zip(prediction, label) if l != -100]\n",
        "        for prediction, label in zip(pred_logits, labels)\n",
        "    ]\n",
        "\n",
        "    true_labels = [\n",
        "      [label_list[l] for (eval_preds, l) in zip(prediction, label) if l != -100]\n",
        "       for prediction, label in zip(pred_logits, labels)\n",
        "   ]\n",
        "    results = metric.compute(predictions=predictions, references=true_labels)\n",
        "\n",
        "    return {\n",
        "          \"precision\": results[\"overall_precision\"],\n",
        "          \"recall\": results[\"overall_recall\"],\n",
        "          \"f1\": results[\"overall_f1\"],\n",
        "          \"accuracy\": results[\"overall_accuracy\"],\n",
        "  }"
      ],
      "metadata": {
        "id": "Jw-EW-EoxJ9u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "trainer = Trainer(\n",
        "   model,\n",
        "   args,\n",
        "   train_dataset=tokenized_datasets[\"train\"],\n",
        "   eval_dataset=tokenized_datasets[\"validation\"],\n",
        "   data_collator=data_collator,\n",
        "   tokenizer=tokenizer,\n",
        "   compute_metrics=compute_metrics\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HHv1pFfkxN6F",
        "outputId": "c4b79094-f3ac-4345-f467-80d684fd7baa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-63-58e691e2f829>:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        },
        "id": "i4ouJ4EmxQzH",
        "outputId": "d66e7598-92aa-4555-d796-ff850a8e9fd7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='3512' max='3512' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [3512/3512 12:51, Epoch 4/4]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.221000</td>\n",
              "      <td>0.063639</td>\n",
              "      <td>0.917046</td>\n",
              "      <td>0.925048</td>\n",
              "      <td>0.921029</td>\n",
              "      <td>0.982207</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.045100</td>\n",
              "      <td>0.060031</td>\n",
              "      <td>0.932979</td>\n",
              "      <td>0.942164</td>\n",
              "      <td>0.937549</td>\n",
              "      <td>0.984924</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.026200</td>\n",
              "      <td>0.055466</td>\n",
              "      <td>0.940058</td>\n",
              "      <td>0.945632</td>\n",
              "      <td>0.942836</td>\n",
              "      <td>0.985893</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.015000</td>\n",
              "      <td>0.058302</td>\n",
              "      <td>0.941046</td>\n",
              "      <td>0.949994</td>\n",
              "      <td>0.945499</td>\n",
              "      <td>0.986481</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=3512, training_loss=0.061158683278294525, metrics={'train_runtime': 771.2843, 'train_samples_per_second': 72.819, 'train_steps_per_second': 4.553, 'total_flos': 1361270260929258.0, 'train_loss': 0.061158683278294525, 'epoch': 4.0})"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"ner_model\")"
      ],
      "metadata": {
        "id": "14-F-LTGxSxd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.save_pretrained(\"tokenizer\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FIRZEaXtyGxc",
        "outputId": "7e8ae9e5-53f4-47ef-b738-3232b8efda6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('tokenizer/tokenizer_config.json',\n",
              " 'tokenizer/special_tokens_map.json',\n",
              " 'tokenizer/vocab.txt',\n",
              " 'tokenizer/added_tokens.json',\n",
              " 'tokenizer/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "id2label = {\n",
        "    str(i): label for i,label in enumerate(label_list)\n",
        "}\n",
        "label2id = {\n",
        "    label: str(i) for i,label in enumerate(label_list)\n",
        "}"
      ],
      "metadata": {
        "id": "Qk1YgMyPyKOk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "id2label"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wlZUI7iyyLoB",
        "outputId": "0001b31c-0a8a-4765-b84a-beef6cc4af83"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'0': 'O',\n",
              " '1': 'B-PER',\n",
              " '2': 'I-PER',\n",
              " '3': 'B-ORG',\n",
              " '4': 'I-ORG',\n",
              " '5': 'B-LOC',\n",
              " '6': 'I-LOC',\n",
              " '7': 'B-MISC',\n",
              " '8': 'I-MISC'}"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "label2id"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KxXgh3J7zzjh",
        "outputId": "845d00cc-4097-4064-c6b8-59eaee7d25cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'O': '0',\n",
              " 'B-PER': '1',\n",
              " 'I-PER': '2',\n",
              " 'B-ORG': '3',\n",
              " 'I-ORG': '4',\n",
              " 'B-LOC': '5',\n",
              " 'I-LOC': '6',\n",
              " 'B-MISC': '7',\n",
              " 'I-MISC': '8'}"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json"
      ],
      "metadata": {
        "id": "326cyA-WyO0M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "config = json.load(open(\"ner_model/config.json\"))\n",
        "config[\"id2label\"] = id2label\n",
        "config[\"label2id\"] = label2id\n",
        "json.dump(config, open(\"ner_model/config.json\",\"w\"))"
      ],
      "metadata": {
        "id": "ovKkbRe6z7EH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_fine_tuned = AutoModelForTokenClassification.from_pretrained(\"ner_model\")"
      ],
      "metadata": {
        "id": "jUUw0x-1z3HI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline"
      ],
      "metadata": {
        "id": "-YUgP5qC0Vml"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = pipeline(\"ner\", model=model_fine_tuned, tokenizer=tokenizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V1eJCP0p0WZA",
        "outputId": "318e9396-9c07-40c6-947e-c2548db5a205"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def result(entities):\n",
        "    formatted_output = {}  # Dictionary to store formatted entities\n",
        "    current_entity = None\n",
        "    current_value = \"\"\n",
        "\n",
        "    for item in entities:\n",
        "        entity_type = item['entity'][2:]  # Remove 'B-' or 'I-' from entity type\n",
        "        word = item['word'].replace(\"##\", \"\")  # Remove subword tokenization artifacts\n",
        "\n",
        "        if item['entity'].startswith(\"B-\"):  # Start of a new entity\n",
        "            if current_entity and current_value:  # Save the previous entity\n",
        "                if current_entity.lower() not in formatted_output:\n",
        "                    formatted_output[current_entity.lower()] = []\n",
        "                formatted_output[current_entity.lower()].append(current_value.strip())\n",
        "            current_entity = entity_type\n",
        "            current_value = word\n",
        "        elif item['entity'].startswith(\"I-\") and entity_type == current_entity:  # Continuation of the same entity\n",
        "            current_value += word\n",
        "\n",
        "    # Append the last entity\n",
        "    if current_entity and current_value:\n",
        "        if current_entity.lower() not in formatted_output:\n",
        "            formatted_output[current_entity.lower()] = []\n",
        "        formatted_output[current_entity.lower()].append(current_value.strip())\n",
        "\n",
        "    # Print the formatted output\n",
        "    for entity_type, values in formatted_output.items():\n",
        "        print(f\"{entity_type}: {', '.join(values)}\")\n"
      ],
      "metadata": {
        "id": "440JvJvE3xpI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example = \"Sridhar will buy Apple phone in India on January 3,2024\"\n",
        "ner_results = nlp(example)\n",
        "result(ner_results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oMDzhyFQ0Z1L",
        "outputId": "f62a5353-70f6-4831-a7d8-8c2ec4a1b0c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "per: sri, dhar\n",
            "org: applephone\n",
            "loc: india\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example = \"On January 15, 2023, John Doe visited the Eiffel Tower in Paris during his vacation. He later attended a conference organized by Google at the Hyatt Regency Hotel. During the event, he met Jane Smith, a software engineer from Microsoft, who was presenting her work on AI models. The event also featured speakers from Stanford University and MIT.\"\n",
        "ner_results = nlp(example)\n",
        "result(ner_results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K9wNpCbq2J37",
        "outputId": "3fce9dbd-32ee-4a8c-c9ad-2d965e20ce20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "per: johndoe, janesmith\n",
            "loc: e, iff, eltower, paris, h, yat, tregencyhotel\n",
            "org: google, microsoft, stanforduniversity, mit\n",
            "misc: aimodels\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example = \"Apple Inc. announced the release of the new iPhone 14 Pro on September 12, 2023, during their keynote in Cupertino, California. Tim Cook, the CEO of Apple, highlighted the phone's advanced A17 Bionic chip and improved camera features. Pre-orders began on September 15, with prices starting at $999 in the United States.\"\n",
        "ner_results = nlp(example)\n",
        "result(ner_results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qq9En68d3ADD",
        "outputId": "a2496ca9-5f3d-4291-add7-58ac48ffe1dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "org: appleinc., apple\n",
            "misc: iphone14pro\n",
            "loc: cup, ert, ino, california, unitedstates\n",
            "per: timcook\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example = \"The World Health Organization (WHO) declared COVID-19 a global pandemic on March 11, 2020. Since then, countries like India, Italy, and the United States have implemented measures such as lockdowns and mass vaccination campaigns. Pharmaceutical companies like Pfizer and Moderna developed vaccines that have been distributed worldwide.\"\n",
        "ner_results = nlp(example)\n",
        "result(ner_results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3pwoLNN546Vu",
        "outputId": "d882b45d-4a89-4f61-d799-c4473008340c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "org: worldhealthorganization, who, p, fi, zer, modern, a\n",
            "misc: co, vid, -, 19\n",
            "loc: india, italy, unitedstates\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_kUguQ1S5TKc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}